{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Conv2D, MaxPool2D, AveragePooling2D, Input, BatchNormalization, MaxPooling2D, Activation, Flatten, Dense, Dropout\nfrom keras.models import Model\nfrom sklearn.metrics import classification_report\nfrom imblearn.over_sampling import RandomOverSampler\nfrom keras.preprocessing import image\nimport scipy\nimport os\nimport cv2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('../input/facial-expression-classifier/fer2013.csv')\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pixel_data = data['pixels']\nlabel_data = data['emotion']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_pixels(pixel_data):\n  images = []\n  for i in range(len(pixel_data)):\n    img = np.fromstring(pixel_data[i], dtype='int', sep=' ')\n    img = img.reshape(48,48,1)\n    images.append(img)\n\n  X = np.array(images)\n\n  return X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"oversampler = RandomOverSampler(sampling_strategy='auto')\n\nX_over, Y_over = oversampler.fit_resample(pixel_data.values.reshape(-1, 1), label_data)\n\nX_over_series = pd.Series(X_over.flatten())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = preprocess_pixels(X_over_series)\nY = Y_over","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_over.values.reshape(Y.shape[0],1)\nY.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size= 0.1, random_state= 45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(X[0, :, :, 0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def emotion_recognition(input_shape):\n\n  X_input = Input(input_shape)\n\n  X = Conv2D(32, kernel_size = (3,3), strides = (1,1), padding='valid')(X_input)\n  X = BatchNormalization(axis = 3)(X)\n  X = Activation('relu')(X)\n    \n  X = Conv2D(64, kernel_size = (3,3), strides = (1,1), padding='same')(X)\n  X = BatchNormalization(axis = 3)(X)\n  X = Activation('relu')(X)\n\n  X = MaxPooling2D((2,2))(X)\n\n  X = Conv2D(64, kernel_size = (3,3), strides = (1,1), padding='valid')(X)\n  X = BatchNormalization(axis = 3)(X)\n  X = Activation('relu')(X)\n\n  X = Conv2D(128, kernel_size = (3,3), strides = (1,1), padding='same')(X)\n  X = BatchNormalization(axis = 3)(X)\n  X = Activation('relu')(X)\n\n\n  X = MaxPooling2D((2,2))(X)\n\n\n  X = Conv2D(128, kernel_size = (3,3), strides = (1,1), padding='valid')(X)\n  X = BatchNormalization(axis = 3)(X)\n  X = Activation('relu')(X)\n\n\n  X = MaxPooling2D((2,2))(X)\n\n  X = Flatten()(X)\n  X = Dense(200, activation = 'relu')(X)\n  X = Dropout(0.6)(X)\n  X = Dense(7, activation = 'softmax')(X)\n\n  model = Model(inputs = X_input, outputs = X)\n\n\n  return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = emotion_recognition((48,48,1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adam = tf.keras.optimizers.Adam(learning_rate = 0.0001)\nmodel.compile(optimizer = adam, loss = 'categorical_crossentropy', metrics = ['accuracy'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train = to_categorical(Y_train, num_classes = 7)\n\ny_test = to_categorical(Y_test, num_classes = 7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, epochs = 30, validation_data = (X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_test, y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = model.predict(X_train)\ndef get_class(preds):\n    pred_class = np.zeros((preds.shape[0],1))\n    for i in range(len(preds)):\n        pred_class[i] = np.argmax(preds[i])\n        \n    return pred_class\n\npred_class_train = get_class(preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_report  = classification_report(Y_train, pred_class_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_report)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_preds = model.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_test_class = get_class(test_preds)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report_test = classification_report(Y_test, pred_test_class)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(report_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_dict = {0: \"Angry\", 1: \"Disguist\", 2: \"Fear\", 3: \"Happiness\", 4: \"Sad\", 5: \"Suprise\", 6: \"Neutral\"}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img_path = plt.imshow(X[0, :, :, 0])\nimg = image.load_img(img_path, grayscale = True, target_size = (48,48))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = image.img_to_array(img)\nx = np.expand_dims(x, axis = 0)\nx.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction = np.argmax(model.predict(x))\nprint(\"The predicted emotion is: \" + label_dict[prediction])\nmy_image = image.load_img(img_path)\nplt.imshow(my_image)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"face_haar_cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cap = cv2. VideoCapture(0)\n\nwhile True:\n    \n    _,cap_image = cap.read()\n    \n    \n    cap_img_gray = cv2.cvtColor(cap_image, cv2.COLOR_BGR2GRAY)\n    \n    faces = face_haar_cascade.detectMultiScale(cap_img_gray, 1.3, 5)\n    \n    for (x,y,w,h) in faces:\n        \n        cv2.rectange(cap_image, (x,y), (x + w, y + h), (255,0,0), 2)\n        roi_gray = cap_img_gray[y:y + h, x:x + h]\n        roi_gray = cv2.resize(roi_gray, (48,48))\n        img_pixels = image.img_to_array(roi_gray)\n        img_pixels = np.expand_dims(img_pixels, axis = 0)\n        \n        predictions = model.predict(img_pixels)\n        emotion_label = np.argmax(predictions)\n        emotion_prediction = label_dict[emotion_label]\n        cv2.putText(cap_image, emotion_prediction, (int(x) int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (255,0,0), 1)\n        resize_image = cv2.resize(cap_image, (1000, 700))\n        cv2.imshow(\"Emotion\", resize_image)\n        if cv2.waitKey(10) == ord('b'):\n            break\ncap.release()\ncv2.destroyALLWindows\n            ","metadata":{},"execution_count":null,"outputs":[]}]}